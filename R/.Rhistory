mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
myStopwords <- c()
mydata <- tm_map(mydata, removeWords, myStopwords)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
inner_join(mantener, by=c("term"="term"))
View(mantener)
mydata %>% TermDocumentMatrix() %>% tidy()
aux <- mydata %>% TermDocumentMatrix() %>% tidy()
View(aux)
mydata %>% TermDocumentMatrix() %>% tidy()
mydata %>% TermDocumentMatrix() %>% tidy()%>%
arrange(desc(document))
mydata %>% TermDocumentMatrix() %>% tidy()%>%
mutate(document = as.integer(document))%>%
arrange(desc(document))
mydata <- Corpus(VectorSource(datos_$message_clean))
mydata
mydata <- Corpus(VectorSource(datos_$message_clean))
mydata %>%TermDocumentMatrix() %>% tidy()%>%
mutate(document = as.integer(document))%>%
arrange(desc(document))
mydata <- Corpus(VectorSource(datos$message_clean))
mydata %>%TermDocumentMatrix() %>% tidy()%>%
mutate(document = as.integer(document))%>%
arrange(desc(document))
mydata <- Corpus(VectorSource(datos$message_clean))
tidata <- mydata %>%TermDocumentMatrix() %>% tidy()
View(tidata)
mydata <- Corpus(VectorSource(datos$message_clean))
tidydata <- mydata %>%TermDocumentMatrix() %>% tidy()
documents <- tidydata%>%
group_by(document)%>%
summarise()%>%
select(document)
View(documents)
mydata <- Corpus(VectorSource(datos$message_clean))
tidydata <- mydata %>%TermDocumentMatrix() %>% tidy()
documents <- tidydata%>%
mutate(document = as.integer(document))%>%
group_by(document)%>%
select(document)
View(mycorpustokenized)
mydata <- Corpus(VectorSource(datos$message_clean))
tidydata <- mydata %>%TermDocumentMatrix() %>% tidy()
documents <- tidydata%>%
mutate(document = as.integer(document))%>%
select(document) %>%
distinct()
View(documents)
tidydata %>%  count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
inner_join(mantener, by=c("term"="term"))
tidydata <- mydata %>%TermDocumentMatrix() %>% tidy()
View(tidydata)
tidydata %>%inner_join(mantener, by=c("term"="term"))
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')
document %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))
)
documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))
)
documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))
)
documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))
)
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))
) %>% mutate(
documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))) %>%
mutate(
term = ifelse(is.na(term), '', term)
)
documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))) %>%
mutate(
term = ifelse(is.na(term), '', term)
) %>%
group_by(document)%>% summarise(vector=paste(term, collapse = " "))
datos
datos$documento <- seq.int(nrow(datos))
clean_twets <- documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term")) %>% filter(keep=='yes')%>%
select(c(term, document))%>%
mutate(document = as.integer(document))) %>%
mutate(
term = ifelse(is.na(term), '', term)
) %>%
group_by(document)%>% summarise(vector=paste(term, collapse = " "))
datos%>%left_join(clean_twets)
View(datos)
View(clean_twets)
datos$document <- seq.int(nrow(datos))
datos%>%left_join(clean_twets, by(c("document" = "document"))
datos$document <- seq.int(nrow(datos))
datos%>%left_join(clean_twets, by=(c("document" = "document"))
clean_twets
datos
datos%>%left_join(clean_twets, by=(c("document" = "document"))
datos$document <- seq.int(nrow(datos))
datos$document <- seq.int(nrow(datos))
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, location, hora, dia, message_clean, vector)
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia)
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia) %>%
write.csv('clear.csv')
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
myStopwords <- c()
mydata <- tm_map(mydata, removeWords, myStopwords)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
inner_join(mantener, by=c("term"="term"))
mydata %>% TermDocumentMatrix() %>% tidy()%>%
mutate(document = as.integer(document))%>%
arrange(desc(document))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
myStopwords <- c()
mydata <- tm_map(mydata, removeWords, myStopwords)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
inner_join(mantener, by=c("term"="term"))
mydata %>% TermDocumentMatrix() %>% tidy()%>%
mutate(document = as.integer(document))%>%
arrange(desc(count))
rm(list=ls())
library(readr)
library(tm)
library(SnowballC)
library(tidyr)
library(dplyr)
library(tidytext)
library(wordcloud)
library(lubridate)
datos <- read_csv("YInt_002.csv") %>%
mutate(
hora = hour(time),
dia = day(time)
)%>%
select(hora, dia, message_clean, location)
mantener <- read_csv("wordcount.csv") %>% filter(keep=="yes") %>% select(term)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 100
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(readr)
library(tm)
library(SnowballC)
library(tidyr)
library(dplyr)
library(tidytext)
library(wordcloud)
library(lubridate)
datos <- read_csv("YInt_002.csv") %>%
mutate(
hora = hour(time),
dia = day(time)
)%>%
select(hora, dia, message_clean, location)
mantener <- read_csv("wordcount.csv") %>% filter(keep=="yes") %>% select(term)
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 100
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 100
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
filter(term == 'fatality')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 100
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
filter(term == 'fatality')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 100
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
filter(term == 'fatalities')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum) %>%
filter(term == 'fatality')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 300
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 300
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)%>%
write.csv('mantener.csv')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mydata <- Corpus(VectorSource(datos$message_clean))
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
minimum <- 300
mydata %>% TermDocumentMatrix() %>% tidy()%>%
count(term) %>%
arrange(desc(n))%>%
filter(n > minimum)%>%
mutate(mantner = 'SI')%>%
write.csv('mantener.csv')
#mydata %>% TermDocumentMatrix() %>% tidy()%>%
#  mutate(document = as.integer(document))%>%
#  arrange(desc(count))
mantener <- read_csv("manter.csv") %>% filter(mantener=="yes") %>% select(term)
mantener <- read_csv("mantener.csv") %>% filter(mantener=="yes") %>% select(term)
read_csv("mantener.csv")
mantener <- read_csv("mantener.csv") %>% filter(mantner=="yes") %>% select(term)
mantener <- read_csv("mantener.csv") %>% filter(mantner=="yes") %>% select(term)
View(mantener)
mantener <- read_csv("mantener.csv")
mantener <- read_csv("mantener.csv") %>% filter(mantner=="si") %>% select(term)
mantener <- read_csv("mantener.csv") %>% filter(mantner=="SI") %>% select(term)
mydata <- Corpus(VectorSource(datos$message_clean))
tidydata <- mydata %>%TermDocumentMatrix() %>% tidy()
documents <- tidydata%>%
mutate(document = as.integer(document))%>%
select(document) %>%
distinct()
clean_twets <- documents %>%left_join(
tidydata %>%inner_join(mantener, by=c("term"="term"))%>%
select(c(term, document))%>%
mutate(document = as.integer(document))) %>%
mutate(
term = ifelse(is.na(term), '', term)
) %>%
group_by(document)%>% summarise(vector=paste(term, collapse = " "))
View(clean_twets)
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia) %>%
write.csv('clear.csv')
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia)
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia) %>%
write.csv('clear.csv')
datos$document <- seq.int(nrow(datos))
datos%>%inner_join(clean_twets, by=(c("document" = "document")))%>%
select(document, message_clean, vector, location, hora, dia) %>%
write.csv('clear.csv')
